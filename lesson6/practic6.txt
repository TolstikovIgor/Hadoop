SQOOP 

1. Получаем список таблиц, которые созданы в postgres (нужно правильно указать ip, сейчас указан localhost)
sqoop list-tables --connect jdbc:postgresql://localhost/pg_db --username exporter --password exporter_pass

3. Запускаем импорт:
sqoop import --connect jdbc:postgresql://localhost/lesson5 --username exporter --password exporter_pass --table test --hive-import --hive-database student3_10 --hive-table test



CREATE TABLE `lesson6_9.character_p`(
	  `charid` string, 
	  `charname` string, 
	  `abbrev` string, 
	  `description` string, 
	  `speechcount` int)
	  PARTITIONed BY (p_date string)
	ROW FORMAT SERDE 
	  'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' 
	WITH SERDEPROPERTIES ( 
	  'field.delim'='\u0001', 
	  'line.delim'='\n', 
	  'serialization.format'='\u0001') 
	STORED AS INPUTFORMAT 
	  'org.apache.hadoop.mapred.TextInputFormat' 
	OUTPUTFORMAT 
	  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
	LOCATION
	  'hdfs://manager.novalocal:8020/user/hive/warehouse/character_p'


DATE=`date '+%Y%m%d'`
sqoop import --connect jdbc:postgresql://node3.novalocal/pg_db --username exporter --password exporter_pass --table character --target-dir /user/hive/warehouse/character_p/p_date=$DATE --delete-target-dir
hive -e 'msck REPAIR TABLE lesson6_9.character_p'


msck REPAIR TABLE lesson6_9.character_p 


FLUME

1. Создаем shell-скрипт и сохраняем в файл heartbeat.sh

START_DATE=`date`
COUNT=0
while [ true ]
do
  NOW_DATE=`date`
  echo I live for $(( (`date -d "$NOW_DATE" +%s` - `date -d "$START_DATE" +%s`) )) seconds\;`(date -d "$START_DATE" +%Y-%m-%d:%H.%M.%S)`\;`(date -d "$START_DATE" +%Y-%m-%d:%H.%M.%S)`\;I did it $(( $COUNT + 1 )) times
  COUNT=$(( $COUNT + 1 ))
  sleep 10
done


2. Создаем конфигурационный файл, прафильное указываем пути до shell-скрипта и hdfs-папки
# Naming the components on the current agent
StudentFlume.sources = Exec   
StudentFlume.channels = MemChannel
StudentFlume.sinks = MySink

# Describing/Configuring the source 
StudentFlume.sources.Exec.type = exec
StudentFlume.sources.Exec.command = /home/centos/flm/heartbeat.sh

# Describing/Configuring the hdfs sink 
StudentFlume.sinks.MySink.type=hdfs
StudentFlume.sinks.MySink.hdfs.path=/user/centos/flm
StudentFlume.sinks.MySink.hdfs.fileSuffix=.log

# Describing/Configuring the channel 
StudentFlume.channels.MemChannel.type = memory
StudentFlume.channels.MemChannel.capacity=10000000
StudentFlume.channels.MemChannel.transactionCapacity = 100

# Bind the source and sink to the channel 
StudentFlume.sources.Exec.channels = MemChannel
StudentFlume.sinks.MySink.channel = MemChannel



3. Запускаем flume (нужно правильно указать пути до конфигурационной папки и конфигурационного файла)
/opt/apache-flume/bin/flume-ng agent --conf /home/student3_10/flm/ --conf-file /home/student3_10/flm/flm.conf --name StudentFlume -Dflume.root.logger=INFO,console


4. Создаем таблицы в hive (найдите ошибку в скрипте, чтобы данные сразу расскладывались по колонкам, укажите правильно location)
create external table flm_logs (first string, second string, third string, fourth string, fifth string)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ';'
STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.SequenceFileInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat'
location '/user/student3_10/flm'

;

5. Проверяем результат
select * from flm_logs


mkdir /downloads/apache-nifi -p
cd /downloads/apache-nifi
wget http://ftp.unicamp.br/pub/apache/nifi/1.9.2/nifi-1.9.2-bin.tar.gz
