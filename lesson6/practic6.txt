SQOOP
1. Получаем список таблиц, которые созданы в postgres (нужно правильно указать ip, сейчас указан
localhost)
sqoop list-tables --connect jdbc:postgresql://localhost/pg_db --username exporter --password exporter_pass
2. Создаем таблицу в hive, куда собираемся выгружать данные
CREATE TABLE `lesson6_9.character`(
`charid` string,
`charname` string,
`abbrev` string,
`description` string,
`speechcount` int)
CREATE TABLE sell_prices
(store_id varchar, item_id varchar, wm_yr_wk varchar, sell_price varchar);
CREATE TABLE `sell_prices`( `store_id` string, `item_id` string, `wm_yr_wk` string, `sell_price` string)
3. Запускаем импорт:
sqoop import --connect jdbc:postgresql://localhost/pg_db --username exporter --password exporter_pass
--table sell_prices --hive-import --hive-database student10_40 --hive-table sell_prices
CREATE TABLE `lesson6.character_p`(
`charid` string,
`charname` string,
`abbrev` string,
`description` string,
`speechcount` int)
PARTITIONed BY (p_date string)
ROW FORMAT SERDE
'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
WITH SERDEPROPERTIES (
'field.delim'='\u0001',
'line.delim'='\n',
'serialization.format'='\u0001')
STORED AS INPUTFORMAT
'org.apache.hadoop.mapred.TextInputFormat'
OUTPUTFORMAT
'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
'hdfs://10.0.0.8:8020/user/hive/warehouse/character_p'
DATE=`date '+%Y%m%d'`
sqoop import --connect jdbc:postgresql://10.0.0.8l/pg_db --username exporter --password exporter_pass
--table character --target-dir /user/hive/warehouse/character_p/p_date=$DATE --delete-target-dir
hive -e 'msck REPAIR TABLE lesson6.character_p'
msck REPAIR TABLE lesson6.character_p
FLUME
1. Создаем shell-скрипт и сохраняем в файл heartbeat.sh
START_DATE=`date`
COUNT=0
while [ true ]
do
NOW_DATE=`date`
echo I live for $(( (`date -d "$NOW_DATE" +%s` - `date -d "$START_DATE" +%s`) )) seconds\;`(date -d
"$START_DATE" +%Y-%m-%d:%H.%M.%S)`\;`(date -d "$START_DATE" +%Y-%m-%d:%H.%M.%S)`\;I did
it $(( $COUNT + 1 )) times
COUNT=$(( $COUNT + 1 ))
sleep 10
Done
2. Создаем конфигурационный файл, правильно указываем пути до shell-скрипта и hdfs-папки
# Naming the components on the current agent
StudentFlume.sources = Exec
StudentFlume.channels = MemChannel
StudentFlume.sinks = MySink
# Describing/Configuring the source
StudentFlume.sources.Exec.type = exec
StudentFlume.sources.Exec.command = /home/centos/flm/heartbeat.sh
# Describing/Configuring the hdfs sink
StudentFlume.sinks.MySink.type=hdfs
StudentFlume.sinks.MySink.hdfs.path=/user/centos/flm
StudentFlume.sinks.MySink.hdfs.fileSuffix=.log
# Describing/Configuring the channel
StudentFlume.channels.MemChannel.type = memory
StudentFlume.channels.MemChannel.capacity=10000000
StudentFlume.channels.MemChannel.transactionCapacity = 100
# Bind the source and sink to the channel
StudentFlume.sources.Exec.channels = MemChannel
StudentFlume.sinks.MySink.channel = MemChannel
3. Запускаем flume (нужно правильно указать пути до конфигурационной папки и конфигурационного
файла)
/opt/apache-flume/bin/flume-ng agent --conf /home/use/flm/ --conf-file /home/user/flm/flm.conf --name
StudentFlume -Dflume.root.logger=INFO,console
4. Создаем таблицы в hive (найдите ошибку в скрипте, чтобы данные сразу раскладывались по
колонкам, укажите правильно location)
create external table lesson10_6.flm_logs (first string, second string, third string, fourth string, fifth string)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.SequenceFileInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat'
location '/user/centos/flm';
5. Проверяем результат
select * from flm_logs
