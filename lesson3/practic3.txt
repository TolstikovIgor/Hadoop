Практическая часть
Для запуска задачи MapReduce нужно выполнить команду
yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar pi 32 10000
Также можно изучить различные демо-задачи, которые поставляются в пакете
hadoop-mapreduce-examples.jar
Для отслеживания задания в yarn нужно зайти на страницу по адресу http://185.241.193.174:8088/.
Нужно найти задачу под своим пользователем и проверить статус выполнения.
Работа задачи MapReduce на популярном примере подсчета повторения слов в файле. Для
выполнение, нужно подготовить входные данные. Загрузить любой файл с помощью команды scp с
локального компьютера или скачать с помощью команды wget. Далее нужно создать отдельную папку
в hdfs, используя команду mkdir:
hdfs dfs -mkdir input
Затем загрузить фаил в hdfs-папку с помощью -put
hdfs dfs -put test.txt input/.
В локальной папке сервера, на котором будет запускаться MapReduce нужно создать два файла
mapper.py и reducer.py. Ниже приведен код этих файлов:
Mapper.py
#!/usr/bin/env python
import sys
for line in sys.stdin:
# удаляем лишние пробелы
line = line.strip()
# делим строки на слова
words = line.split()
# добавляем значение для счетчика
for word in words:
# выводим в output пару ключ и значение
print '%s\t%s' % (word, 1)
Reducer.py
#!/usr/bin/env python
import sys
current_word = None
current_count = 0
word = None
# читаем строки из STDIN
for line in sys.stdin:
# удаляем пробелы в конце и начале строки
line = line.strip()
# разделяем пары ключ и значение
word, count = line.split('\t', 1)
# преобразуем значение в число
try:
count = int(count)
except ValueError:
# игнорируем ошибки
continue
# на Reduce приходят данные после фазы Sort
# поэтому этот код будет работать
if current_word == word:
current_count += count
else:
if current_word:
# пишем результат в STDOUT
print '%s\t%s' % (current_word, current_count)
current_count = count
current_word = word
# не забываем про последнее слово
if current_word == word:
print '%s\t%s' % (current_word, current_count)
Код в mapper.py и reducer.py написан для выполнения с python3. Нужно помнить об этом и если в
системе стоит отличная версия python, то точно указывать с каким интерпретатором запускать.
Локальное тестирования вычисления производится следующей командой:
cat test.txt | python mapper.py | sort | python reducer.py
Для запуска распределенного вычисления нужно воспользоваться следующей командой:
yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar -input test -output result
-mapper "python mapper.py" -reducer "python reducer.py" -file mapper.py -file reducer.py
Обзор результата
hdfs dfs -cat result/*
Для повторного запуска нужно не забыть удалить старый результат командой:
hdfs dfs -rm -r result
